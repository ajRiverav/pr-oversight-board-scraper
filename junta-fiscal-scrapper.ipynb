{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests               #to request list of documents in htlm format\n",
    "from bs4 import BeautifulSoup #to parse html\n",
    "from datetime import datetime #to convert date to ISO8601\n",
    "import pandas as pd           #to save it as csv. \n",
    "import urllib.request         #to download documents\n",
    "import time                   #to give doc downloaded time to operate\n",
    "import os                     #to create dirs and check existing ones\n",
    "import shutil                 #to delete, move dirs\n",
    "import csvdiff\n",
    "import json                   #to open json files\n",
    "\n",
    "# TODO:\n",
    "# Change use of pandas to save to a csv file (use csv module instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "DEBUG_FLAG = True\n",
    "SAVE_DOCS = False\n",
    "NEW_DIR = \"./new-downloaded-docs/\"\n",
    "OLD_DIR = \"./old-downloaded-docs/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading website:\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading website:\")\n",
    "if DEBUG_FLAG:\n",
    "    r = open('./test-files/docs-day-2.html')\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "else:\n",
    "    tries_count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get('https://juntasupervision.pr.gov/index.php/en/documents/')\n",
    "            break\n",
    "        except:\n",
    "            tries_count += 1\n",
    "            if tries_count>10:\n",
    "                print(\"Already tried 10 times...I'm giving up. Sorry.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Could not access the website. Trying again...\")\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Documents are placed in each row, so go straight to it\n",
    "\n",
    "doc_list_soup = soup.findAll(\"div\",{\"class\": \"doc-row\"});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store every document in a table row\n",
    "\n",
    "table = []\n",
    "\n",
    "for doc in doc_list_soup:\n",
    "    title = doc.a.getText()\n",
    "    \n",
    "    category = doc.find(\"div\",{\"class\": \"span2 cat\"}).getText()\n",
    "    \n",
    "    tmp_date = doc.find(\"div\",{\"class\": \"span2 date\"}).getText()\n",
    "    date = str(datetime.strptime(tmp_date, '%b.%d.%Y')) # convert to ISO-8601 date\n",
    "    \n",
    "    download_url = doc.a.get('href')\n",
    "    \n",
    "    download_title = doc.a.get('download')\n",
    "    \n",
    "    table.append( [title,category,date,download_url,download_title] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the last downloaded data to the old directory so that the new\n",
    "# data goes in the new directory. \n",
    "\n",
    "new_dir_exists = os.path.isdir(NEW_DIR)\n",
    "old_dir_exists = os.path.isdir(OLD_DIR)\n",
    "\n",
    "if new_dir_exists: #if new dir exists\n",
    "    # move to the old dir, but delete current content of old dir first\n",
    "    if old_dir_exists:\n",
    "        for the_file in os.listdir(OLD_DIR):\n",
    "            file_path = os.path.join(OLD_DIR, the_file)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.unlink(file_path)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    for the_file in os.listdir(NEW_DIR):\n",
    "        file_path = os.path.join(NEW_DIR, the_file)\n",
    "        shutil.move(file_path, os.path.join(OLD_DIR,the_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data to a csv file\n",
    "\n",
    "csv_filename = 'doc-table.csv'\n",
    "\n",
    "df = pd.DataFrame(table)\n",
    "df.columns = ['title','category','date','download_url','download_title']\n",
    "df.to_csv(NEW_DIR+csv_filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download documents\n",
    "\n",
    "if SAVE_DOCS:\n",
    "    num_files = len(df.download_url)\n",
    "\n",
    "    for url,i in zip(df.download_url, range(num_files)):\n",
    "        fn = url.split('/')[-1]    \n",
    "        tries_count = 0\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                print(\"Downloading file #\" + str(i+1) + \"/\" + str(num_files) + \" - \" + fn)\n",
    "                urllib.request.urlretrieve(url, filename=NEW_DIR+fn)\n",
    "                break\n",
    "            except:\n",
    "                tries_count += 1\n",
    "                if tries_count>10:\n",
    "                    print(\"Already tried 10 times...I'm giving up. Sorry\")\n",
    "                else:\n",
    "                    print(\"Could not download file. Trying again.\")\n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"_index\": [\n",
      "        \"title\"\n",
      "    ],\n",
      "    \"added\": [\n",
      "        {\n",
      "            \"category\": \"General Release\",\n",
      "            \"date\": \"2018-01-24 00:00:00\",\n",
      "            \"download_title\": \"General Release Announcing Additional Public Comment Period for CPP Projects.pdf\",\n",
      "            \"download_url\": \"https://juntasupervision.pr.gov/wp-content/uploads/wpfd/49/5a690f48694e6.pdf\",\n",
      "            \"title\": \"General Release Announcing Additional Public Comment Period for CPP Projects\"\n",
      "        }\n",
      "    ],\n",
      "    \"changed\": [],\n",
      "    \"removed\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check differences and store them in a json file format\n",
    "\n",
    "json_filename = \"differences.json\"\n",
    "try:\n",
    "    os.remove(json_filename)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.system(\"csvdiff --style=pretty --output=\" + json_filename + \" title \" + \\\n",
    "          OLD_DIR + \"doc-table.csv \" + \\\n",
    "          NEW_DIR + \"doc-table.csv\")\n",
    "\n",
    "with open(json_filename) as json_data:\n",
    "    d = json.load(json_data)\n",
    "    print(json.dumps(d, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
